  
---

### **Diseño de la Sesión: "El Horizonte Ético de la IA en el Sector Público y la Academia"**

**Duración:** 30 minutos **Formato:** Círculo de Discusión Guiada (sin diapositivas) **Objetivo:** Facilitar una reflexión colectiva sobre los dilemas éticos más pertinentes para la comunidad de la Escuela de Gobierno.

---

#### **Estructura de la Sesión (30 Minutos)**

[https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/](https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/)

[https://www.brainonllm.com/](https://www.brainonllm.com/)

Paper: [https://arxiv.org/pdf/2506.08872](https://arxiv.org/pdf/2506.08872)

The methodology of this paper contains several notable limitations in terms of participant recruitment, experimental design, and measurement scope.

**1. Sample and Demographics:**

- The **sample size was limited**, particularly for the crucial Session 4, which only included 18 participants. Analysis of Session 4 results is thus considered preliminary.
    
- Participants (N=54) were **recruited from a single specific demographic**—adults from five major universities in the greater Boston area. The findings may not generalize to diverse populations, such as different age groups or professionals.
    
- Future work should aim for a **more gender-balanced population**.
    

**2. Experimental Design and Generalizability:**

- The study **only utilized one specific Large Language Model (LLM)**, OpenAI's GPT-4o, limiting the ability to generalize the results to other commercially available LLM models.
    
- The core task, essay writing, **was not segmented into distinct subtasks** (such as idea generation or revision), which would have allowed for a more in-depth, stage-specific analysis of neural activity.
    
- The findings are **context-dependent**, focusing specifically on essay writing in an educational setting, meaning they may not be applicable across different cognitive tasks.
    

**3. Measurement Limitations (EEG Data):**

- The EEG analysis focused exclusively on **connectivity patterns (dDTF) but excluded spectral power changes**, which would have provided supplementary insights into neural efficiency.
    
- The **spatial resolution of EEG is limited**, preventing the precise localization of activity in deeper cortical or subcortical regions (such as the hippocampus). The authors suggest fMRI for future work to address this limitation.
    
- **Electrooculography (EOG) data was recorded but excluded** from the final manuscript.
    

1. ### Is it safe to say that LLMs are, in essence, making us "dumber"?
    
    No! Please do not use the words like “stupid”, “dumb”, “brain rot”, "harm", "damage", "brain damage", "passivity", "trimming" , "collapse" and so on. It does a huge disservice to this work, as we did not use this vocabulary in the paper, especially if you are a journalist reporting on it.
    
2. ### Do you have a peer review timeline for this project?
    
    Currently this paper is a preprint. We decided to release this preprint now to collect a wider feedback faster as the topic is pressing and the speed of development and integration of LLMs in everyday lives is unmatched and not something we have truly seen before.
    
    Peer review process has been already started, but we are in the very beginning of this process, and it will probably take months. As you might know/have heard/read – peer reviews take usually a long period of time, anytime between 4 months and up two years.
    
3. ### Are you planning any additional studies in the near future?
    
    Yes, the next one is about the "vibe coding". We have already collected the data and are currently working on the analysis and draft. It adds to why it is important to get general public's feedback now.
    
4. ### Anything else to add that you feel is relevant to a story about this project?
    
    Lots of media and people used LLMs to summarize the paper. It adds to the noise. Your HUMAN feedback is very welcome, if you read the paper or parts of it.
    
    Also, as a reminder, the study has a list of limitations we state very clearly both in the paper and on the webpage. And to the best of our knowledge, it is one of the first protocols of its kind, thus we do expect more papers/studies (from ourselves and other researchers!) with different protocols, populations, tasks, methodologies, that will add to the general understanding of the use of this technology in different aspects of our lives.
    
5. ### Additional vocabulary to avoid using when talking about the paper
    
    In addition to the vocabulary from Question 1 in this FAQ - please avoid using "brain scans", "LLMs make you stop thinking", "impact negatively", "brain damage", "terrifying findings".
    

Additionally, there are several **limitations** and important avenues for future work, which will need to be addressed in the next or similar studies:

1. In this study we had a **limited number of participants** recruited from a specific geographical area, several large academic institutions, located very close to each other. For future work it will be important to include a larger number of participants coming with diverse backgrounds like professionals in different areas, age groups, as well as ensuring that the study is more gender balanced.
    
2. This study was performed using ChatGPT, and though we do not believe that as of the time of this paper publication in June 2025, there are any significant breakthroughs in any of the commercially available models to grant a significantly different result, we cannot directly generalize the obtained results to other LLM models. Thus, for future work it will be important to include several LLMs and/or offer users a choice to use their preferred one, if any.
    
3. Future work may also include the use of LLMs with other modalities beyond the text, like audio modality.
    
4. We did not divide our essay writing task into subtasks like idea generation, writing, and so on, which is often done in prior work. This labeling can be useful to understand what happens at each stage of essay writing and have more in-depth analysis.
    
5. In our current EEG analysis we focused on reporting connectivity patterns without examining spectral power changes, which could provide additional insights into neural efficiency. EEG's spatial resolution limits precise localization of deep cortical or subcortical contributors (e.g. hippocampus), thus fMRI use is the next step for our future work.
    
6. Our findings are **context-dependent and are focused on writing an essay in an educational setting and may not generalize across tasks.**
    
7. Future studies should also consider **exploring longitudinal impacts** of tool usage on memory retention, creativity, and writing fluency.
    

**(1) Introducción y Preparación del Espacio (3 minutos)**

- **Acción del Facilitador:** Invita a los participantes a formar un gran círculo con sus sillas. Agradece su energía y participación durante todo el taller.
    
- **Guion del Facilitador:**
    
    > "Para cerrar nuestro taller, no vamos a tener una última lección, sino una conversación. La tecnología que hemos explorado es poderosa, y con el poder viene la responsabilidad. Esta media hora es un espacio para reflexionar juntos, como comunidad, sobre los dilemas éticos que ya enfrentamos y los que enfrentaremos.
    > 
    > No hay respuestas correctas o incorrectas, solo perspectivas valiosas. La regla es simple: escuchar para entender, no para responder. Agradezco de antemano su apertura."
    

**(2) Horizonte 1: Integridad y Autoría (10 minutos)**

- **Pregunta detonadora:**
    
    > _Un colega utiliza IA para generar el 80% de un reporte de política pública crucial. El trabajo es excelente, bien investigado y se entregó en tiempo récord. Lo presenta como 100% suyo. La organización se beneficia de la eficiencia y la calidad._
    > 
    > **La pregunta es:** ¿Existe un problema ético aquí? ¿Por qué sí o por qué no? ¿Dónde dibujamos la línea entre usar una herramienta para aumentar la productividad y la integridad autoral?"
    

### **(3) Horizonte 2: El Dilema de la Política Institucional - Equidad y Transparencia (10 minutos)**

- **Pregunta detonadora:**
    
    "Ahora, traigamos el dilema a casa. De nuestro trabajo individual, pasemos al diseño de políticas para nuestros propios estudiantes. Consideren este escenario:
    
    - *La Escuela de Gobierno decide implementar un sistema de IA para identificar de manera proactiva a los estudiantes 'en riesgo académico'. *
        
    - *El sistema analiza decenas de variables: calificaciones, asistencia, participación en clase (medida por transcripciones de IA), patrones de entrega de tareas y hasta datos demográficos y socioeconómicos (con el objetivo de identificar barreras externas). *
        
    - *El sistema funciona y predice con alta precisión qué estudiantes podrían reprobar. Como resultado, se les asigna un tutor de apoyo obligatorio. *
        
    - _Sin embargo, los estudiantes etiquetados sienten que han sido 'perfilados' y estigmatizados por un algoritmo que no entienden, y algunos argumentan que la 'ayuda' no solicitada afecta negativamente su autonomía y motivación._
        
    - La pregunta es:**
        
        - ¿Es ético implementar un sistema de vigilancia predictiva, incluso si la intención es ayudar?
            
        - ¿Qué derechos tienen los estudiantes sobre sus datos y sobre la 'explicabilidad' del algoritmo que los evalúa? Y,
            
        - ¿cómo equilibramos el objetivo institucional de mejorar las tasas de éxito académico con el respeto a la autonomía y la privacidad del estudiante?"
            

---

### **(3) Horizonte 2: El Dilema del Aula - Honestidad Académica y la Nueva Realidad (10 minutos)**

- **Objetivo:** Abordar directamente el conflicto entre las definiciones tradicionales de plagio y las nuevas capacidades de la IA generativa en el contexto de la evaluación estudiantil.
    
- **Acción del Facilitador:** Presenta un escenario que pone en tensión las políticas académicas vigentes.
    
- **Guion del Facilitador:**
    
    > "Ahora, llevemos el debate al corazón de nuestra misión educativa: el aula. Todos aquí estamos involucrados en la formación y evaluación de estudiantes. Consideren este escenario real:
    > 
    > _Para un ensayo final, dos estudiantes entregan trabajos excelentes. Ambos obtienen la misma calificación._
    > 
    > - **El Estudiante A** pasó 30 horas investigando en la biblioteca, estructurando sus ideas y redactando meticulosamente cada párrafo. Su trabajo es 100% original en el sentido tradicional.*
    >     
    > - **El Estudiante B** pasó 30 horas, pero de una manera diferente. Dedicó 10 horas a construir un 'agente de investigación' de IA altamente sofisticado, entrenándolo con prompts complejos para encontrar, sintetizar y debatir las mejores fuentes. Luego, dedicó 20 horas a iterar con la IA, actuando como un 'director de orquesta': guiando, criticando, editando y refinando el texto generado por el modelo hasta alcanzar un nivel de calidad excepcional.
    >     
    > - _*El trabajo del Estudiante B no podría haber sido creado sin la IA, pero tampoco sin su profundo dominio de la herramienta y su intenso trabajo de dirección y edición._
    >     
    > 
    > **La pregunta es:** ¿Han hecho trampa alguno de los dos? ¿Es el trabajo del Estudiante B menos meritorio que el del Estudiante A? Si nuestra política de honestidad académica prohíbe el uso de texto 'no original', ¿estamos penalizando injustamente una nueva forma de competencia y habilidad que será crucial en su futuro profesional? ¿Cómo debería evolucionar nuestra definición de 'trabajo original' y qué deberíamos estar evaluando realmente: el producto final o el proceso de pensamiento?"
    

**(4) Horizonte 3: El Dilema del Co-Piloto - El Futuro del Juicio Humano y Cierre (7 minutos)**

- **Pregunta detonadora:**
    
    > "Para terminar, miremos al horizonte. Hemos visto cómo la IA pasa de ser una herramienta a un agente, un verdadero 'co-piloto' en nuestro trabajo.
    > 
    > _Mirando a 5 o 10 años en el futuro, a medida que estos agentes se vuelvan omnipresentes en la investigación, la docencia y la gestión pública, y hagan gran parte del trabajo analítico por nosotros..._
    > 
    > **La pregunta final es:** ¿Cuál es la habilidad humana más importante que debemos proteger, cultivar y enseñar para no volvernos meros 'supervisores de máquinas'? ¿Y cuál es la responsabilidad específica de una institución como la Escuela de Gobierno y el tec-monterrey en esa tarea?"
    

## Los retos éticos críticos que definen nuestra década: Un análisis más allá de las narrativas convencionales

Permíteme compartir contigo un análisis que va más allá de las preocupaciones superficiales sobre "IA malvada" o "privacidad en redes sociales". Los verdaderos retos éticos que enfrentamos son más sutiles, sistémicos y peligrosos precisamente porque operan bajo el radar del discurso público.

### El contexto que nos trajo aquí

Antes de sumergirnos en los retos específicos, es crucial entender que estamos viviendo en un momento de transición fundamental. No es simplemente que la tecnología avance rápidamente; es que las estructuras básicas de cómo organizamos la sociedad, distribuimos recursos y tomamos decisiones están mutando a una velocidad que supera nuestra capacidad de crear marcos éticos apropiados. Imagina intentar escribir las reglas de tránsito mientras los vehículos evolucionan de carruajes a aviones en cuestión de meses - ese es nuestro dilema actual.

## Los retos éticos de 2025: Las crisis silenciosas ya en marcha

### 1. La erosión de la agencia humana mediante la optimización algorítmica del comportamiento

No me refiero al miedo simplista de que "los algoritmos controlan nuestras mentes". El problema real es mucho más insidioso: hemos creado sistemas que son tan efectivos prediciendo y moldeando el comportamiento humano que la línea entre decisión autónoma y manipulación sutil se ha vuelto indistinguible.

Considera cómo funciona esto en la práctica. Los sistemas de recomendación no solo predicen qué video verás después; están optimizando para maximizar tu tiempo de engagement, lo cual correlaciona fuertemente con contenido que provoca respuestas emocionales extremas. Pero aquí está el giro ético profundo: estos sistemas ahora son tan sofisticados que pueden identificar y explotar vulnerabilidades psicológicas individuales. Una persona con tendencias depresivas recibe contenido diferente que alguien con ansiedad, no para ayudarles, sino para maximizar su engagement.

El dilema ético real no es si esto es manipulación (lo es), sino que hemos creado una economía global donde las empresas más valiosas del mundo dependen fundamentalmente de esta erosión de la agencia humana para su modelo de negocio. Es como si hubiéramos descubierto que el plomo en la gasolina causa daño neurológico, pero toda nuestra infraestructura de transporte dependiera de él.

### 2. La crisis de verificación epistémica en la era de la síntesis perfecta

Para 2025, la capacidad de generar contenido sintético indistinguible de la realidad no es ciencia ficción - es una herramienta disponible en tu smartphone. Pero el problema ético trasciende las "deepfakes" políticas que todos temen.

El verdadero reto es que estamos destruyendo lo que llamo la "cadena de custodia epistémica" - la capacidad de rastrear el origen y veracidad de la información. Imagina este escenario que ya está ocurriendo: un investigador usa IA para generar datos sintéticos para entrenar otro modelo de IA, que luego genera "insights" que se publican en journals académicos. Otro investigador cita estos papers, y gradualmente, construimos capas sobre capas de "conocimiento" que no tiene conexión verificable con la realidad empírica.

Es como construir un rascacielos donde cada piso está basado en hologramas del piso anterior. Eventualmente, todo el edificio del conocimiento humano corre el riesgo de convertirse en un simulacro - una copia sin original.

### 3. El apartheid cognitivo y la estratificación de la inteligencia aumentada

Aquí hay algo que pocos discuten abiertamente: estamos en las primeras etapas de una división fundamental de la humanidad en dos clases cognitivas. No es sobre quién tiene acceso a ChatGPT (eso es trivial), sino sobre quién tiene acceso a sistemas de aumentación cognitiva verdaderamente poderosos y personalizados.

Los ejecutivos de alto nivel ya tienen asistentes de IA entrenados específicamente en sus patrones de pensamiento, con acceso a datos propietarios y capacidades que el público ni siquiera sabe que existen. Un trader en Wall Street con acceso a modelos propietarios puede procesar información y tomar decisiones a un nivel cualitativamente diferente que alguien usando herramientas públicas.

Es como si algunos humanos hubieran evolucionado telepáticamente mientras otros siguen usando señales de humo. La brecha no es solo de productividad; es una divergencia fundamental en la capacidad cognitiva efectiva. Y aquí está el dilema ético: ¿es justo competir por trabajos, recursos o poder político cuando algunos humanos operan con el equivalente a un exocerebro mientras otros no?

### 4. La responsabilidad diluida en sistemas de decisión distribuidos

Los sistemas de IA no toman decisiones en aislamiento; están embebidos en redes complejas donde ningún humano entiende completamente la cadena causal. Cuando un algoritmo niega un préstamo, ¿quién es responsable? ¿El programador que escribió el código hace tres años? ¿El científico de datos que seleccionó las variables? ¿El gerente que estableció los KPIs? ¿La empresa que proporcionó los datos de entrenamiento?

Es como un pelotón de fusilamiento donde cada soldado tiene un rifle pero nadie sabe quién tiene las balas reales. La difusión de responsabilidad no es un bug - es una característica que protege a las organizaciones de la accountability. El resultado es que decisiones que impactan profundamente vidas humanas (libertad condicional, diagnósticos médicos, admisiones universitarias) se toman en una zona gris ética donde nadie es verdaderamente responsable.

## Los retos éticos de 2030: Las crisis emergentes que nos tomarán por sorpresa

### 5. La obsolescencia selectiva de la experiencia humana

Para 2030, no enfrentaremos un "desempleo masivo por IA" como predicen los titulares. El problema será más cruel: la obsolescencia selectiva y aparentemente aleatoria de dominios completos de expertise humana.

Imagina ser un radiólogo con 30 años de experiencia, despertando un martes para descubrir que un nuevo modelo de IA no solo hace tu trabajo mejor, sino que revela que todo tu campo ha estado operando bajo premisas fundamentalmente erróneas. No es solo que pierdes tu trabajo; es que tu identidad profesional, tu sentido de contribución al mundo, se evapora instantáneamente.

Pero aquí está el giro: esto no ocurrirá uniformemente. Algunos campos aparentemente "seguros" colapsarán overnight, mientras otros supuestamente vulnerables persistirán por razones no técnicas sino sociopolíticas. La ética de decidir qué experiencia humana "merece" ser preservada artificialmente (como hicimos con la industria del carbón) versus cual se deja morir será brutal.

### 6. El problema de la coherencia temporal del yo en realidades editables

Para 2030, la tecnología de realidad mixta y los interfaces cerebro-computadora tempranos permitirán algo que nunca hemos enfrentado: la capacidad de editar selectivamente nuestra percepción y memoria de la realidad en tiempo real.

No hablo de "borrar recuerdos traumáticos" como en las películas. Hablo de sistemas que pueden sutilmente ajustar tu percepción del pasado para hacerte más productivo, menos ansioso, o más compatible con las necesidades de tu empleador. Imagina una empresa ofreciendo un "paquete de bienestar" que incluye ajustes cognitivos que te hacen recordar tus experiencias laborales más positivamente de lo que fueron.

El dilema ético fundamental: si puedes editar tu experiencia subjetiva para ser más feliz, ¿deberías hacerlo? ¿Y qué sucede con el concepto de "autenticidad" cuando tu yo de ayer es literalmente una versión editada? Es como el dilema del barco de Teseo, pero aplicado a la consciencia humana.

### 7. La crisis de legitimidad democrática en la era de los votantes sintéticos

Este es el reto que nadie quiere discutir: para 2030, será técnicamente posible crear millones de "ciudadanos digitales" - entidades de IA que pueden pasar cualquier test de Turing, tienen historiales coherentes en redes sociales, y pueden participar en discusiones políticas de maneras indistinguibles de humanos reales.

El problema no es solo "bots difundiendo desinformación". Es que la distinción entre opinión pública genuina y manufactura sintética de consenso se volverá imposible de determinar. Si el 60% de los "ciudadanos" apoyando una política son sintéticos, ¿es esa política legítima? ¿Cómo mantienes la democracia cuando no puedes verificar que los participantes en el proceso democrático sean humanos reales?

Es como intentar tener una elección donde no solo no sabes quién vota, sino que no puedes estar seguro de que los votantes existan. La democracia, como la conocemos, depende de la premisa de "un humano, un voto". Cuando esa premisa se vuelve inverificable, todo el sistema colapsa.

### 8. El colapso de la biodiversidad conductual humana

Aquí hay algo profundamente perturbador que está emergiendo: a medida que más humanos dependen de sistemas de IA para decisiones (qué comer, qué ruta tomar, cómo responder a un email), estamos convergiendo hacia un conjunto cada vez más estrecho de comportamientos "óptimos".

Es como la evolución en reversa. En lugar de diversificación, estamos viendo homogenización. Todos recibimos las mismas recomendaciones de los mismos algoritmos, creando una mono-cultura conductual. El peligro no es solo la pérdida de individualidad; es que la diversidad conductual es lo que hace a las sociedades resilientes. Una población donde todos piensan y actúan de manera similar es extremadamente vulnerable a shocks sistémicos.

Para 2030, podríamos enfrentar el equivalente conductual de la hambruna irlandesa de la papa - donde la sobre-dependencia en una sola variedad creó vulnerabilidad catastrófica.

## Los meta-retos que conectan todo

### El problema de la velocidad versus la sabiduría

El reto ético más fundamental que enfrentamos es que la velocidad del cambio tecnológico opera en escalas de tiempo de días o semanas, mientras que la sabiduría ética emerge en escalas de generaciones. Es como intentar desarrollar regulaciones de seguridad para aviones mientras estos evolucionan más rápido de lo que puedes escribir las reglas.

La tentación es decir "pausemos la tecnología hasta que la ética la alcance", pero eso es naive. La tecnología no es una fuerza monolítica que puedas pausar; es un proceso emergente distribuido globalmente. Cualquier región que intente "pausar" simplemente será superada por otras que no lo hagan.

### La paradoja de la prevención invisible

Aquí está el cruel dilema final: los retos éticos más peligrosos son aquellos que, si los prevenimos exitosamente, nunca sabremos que existieron. Es como la paradoja del Y2K - el éxito en la prevención parece paranoia en retrospectiva.

Esto crea un incentivo perverso: es políticamente más rentable reaccionar a crisis que prevenirlas. Un político que previene un colapso sistémico que nunca ocurre no recibe crédito; uno que "heroicamente" responde a una crisis que pudo haberse prevenido es celebrado.

## Qué deberíamos realmente temer

No deberíamos temer que la IA se vuelva consciente y decida eliminarnos. Deberíamos temer algo mucho más mundano y, por lo tanto, más probable: que construyamos gradualmente una prisión dorada de conveniencia y optimización de la cual no podamos escapar porque cada barra individual parece racional y beneficiosa.

Es la muerte por mil algoritmos útiles, cada uno mejorando marginalmente nuestras vidas mientras, colectivamente, erosionan las fundaciones de la agencia humana, la autenticidad y la autonomía.

El verdadero reto ético de nuestro tiempo no es prevenir el apocalipsis de la IA. Es preservar lo que significa ser humano en un mundo donde cada aspecto de esa humanidad puede ser ampliado, optimizado o sintetizado. Y lo más aterrador es que podríamos perder esa batalla no a través de la coerción, sino a través de nuestra propia aquiescencia voluntaria, intercambiando nuestra humanidad por conveniencia, un algoritmo a la vez.

La pregunta que deberíamos hacernos no es "¿Puede la IA reemplazar a los humanos?" sino "¿Qué aspectos de la experiencia humana estamos dispuestos a defender como irreemplazables, incluso si la alternativa sintética es 'mejor' por cualquier métrica medible?"

Porque al final, ese podría ser el reto ético más importante de todos: definir qué valores humanos son tan fundamentales que los preservaríamos incluso a costa de la eficiencia, la optimización y el progreso medible. Y tener el coraje de defenderlos cuando llegue el momento.